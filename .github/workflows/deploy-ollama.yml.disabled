name: ðŸ¤– Deploy Ollama to VM

on:
  workflow_dispatch:
    inputs:
      force_reinstall:
        description: 'Force reinstall Ollama'
        required: false
        default: false
        type: boolean

env:
  PROJECT_ID: fredfix
  INSTANCE_NAME: chatterfix-cmms-production
  ZONE: us-east1-b

jobs:
  deploy-ollama:
    name: ðŸ¤– Setup Ollama on Production VM
    runs-on: ubuntu-latest
    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ðŸ”‘ Setup GCP Authentication
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: â˜ï¸ Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: ðŸš€ Deploy Ollama to VM
        run: |
          echo "ðŸ¤– Deploying Ollama to production VM..."
          
          # Copy optimization script to VM
          gcloud compute scp scripts/optimize-ollama.sh ${{ env.INSTANCE_NAME }}:/tmp/optimize-ollama.sh --zone=${{ env.ZONE }}
          
          # Execute Ollama setup on VM
          gcloud compute ssh ${{ env.INSTANCE_NAME }} --zone=${{ env.ZONE }} --command="
            chmod +x /tmp/optimize-ollama.sh
            echo 'ðŸ¤– Starting Ollama setup on VM...'
            sudo /tmp/optimize-ollama.sh
            echo 'âœ… Ollama setup complete on VM'
          "
          
          echo "âœ… Ollama deployment completed"

      - name: ðŸ”§ Configure CMMS for VM Ollama
        run: |
          echo "ðŸ”§ Configuring CMMS to use VM-based Ollama..."
          
          gcloud compute ssh ${{ env.INSTANCE_NAME }} --zone=${{ env.ZONE }} --command="
            cd /opt/chatterfix-cmms
            
            # Ensure Ollama URL points to VM localhost
            if [ -f app.py ]; then
              sudo sed -i 's/localhost:11434/127.0.0.1:11434/g' app.py
              echo 'âœ… Updated app.py Ollama configuration'
            fi
            
            # Check for other config files that might need updating
            for file in *.py; do
              if [ -f \"\$file\" ] && grep -q '11434' \"\$file\"; then
                echo \"ðŸ“ Found Ollama references in \$file\"
              fi
            done
            
            # Restart CMMS service to pick up changes
            echo 'ðŸ”„ Restarting CMMS service...'
            sudo systemctl restart chatterfix-cmms
            echo 'âœ… CMMS service restarted'
          "

      - name: ðŸ¥ Verify Ollama on VM
        run: |
          echo "ðŸ¥ Verifying Ollama installation on VM..."
          
          gcloud compute ssh ${{ env.INSTANCE_NAME }} --zone=${{ env.ZONE }} --command="
            echo 'ðŸ“Š Ollama service status:'
            sudo systemctl status ollama --no-pager || true
            echo ''
            
            echo 'ðŸ” Testing Ollama API:'
            if curl -s http://localhost:11434/api/tags >/dev/null 2>&1; then
              echo 'âœ… Ollama API is responding on VM'
              curl -s http://localhost:11434/api/tags | head -3
            else
              echo 'âš ï¸ Ollama API not responding - checking logs...'
              sudo journalctl -u ollama --no-pager -n 10
            fi
            echo ''
            
            echo 'ðŸ¤– Available models:'
            sudo -u ollama ollama list || echo 'No models available yet'
            echo ''
            
            echo 'ðŸ” CMMS service status:'
            sudo systemctl status chatterfix-cmms --no-pager || true
          "

      - name: ðŸ§ª Test LLaMA Integration
        run: |
          echo "ðŸ§ª Testing LLaMA integration..."
          
          # Test if the health endpoint shows LLaMA is available
          if curl -s https://chatterfix.com/health | grep -q "Llama"; then
            echo "âœ… LLaMA is reported as available in health endpoint"
          else
            echo "âš ï¸ LLaMA not showing in health endpoint"
          fi
          
          # Test VM Ollama directly through SSH
          gcloud compute ssh ${{ env.INSTANCE_NAME }} --zone=${{ env.ZONE }} --command="
            echo 'ðŸ¤– Testing LLaMA model on VM:'
            if sudo -u ollama ollama run llama3.1:8b 'Hello from VM!' 2>/dev/null; then
              echo 'âœ… LLaMA model responding on VM'
            else
              echo 'ðŸ“¥ LLaMA model may need to be pulled...'
              sudo -u ollama ollama pull llama3.1:8b
            fi
          "

      - name: ðŸ“Š Deployment Report
        run: |
          echo "ðŸ“Š Ollama VM Deployment Report"
          echo "=============================="
          echo "VM: ${{ env.INSTANCE_NAME }}"
          echo "Zone: ${{ env.ZONE }}"
          echo "Timestamp: $(date)"
          echo ""
          echo "ðŸ”— Test the AI assistant at: https://chatterfix.com"
          echo "ðŸ¤– Floating AI icon should now use VM-based LLaMA"
          echo ""
          echo "ðŸ“‹ To monitor:"
          echo "  gcloud compute ssh ${{ env.INSTANCE_NAME }} --zone=${{ env.ZONE }} --command='journalctl -u ollama -f'"