name: ğŸ¤– Install Best Ollama Model for ChatterFix AI

on:
  push:
    paths:
      - '.github/workflows/install-best-ollama-model.yml'
  workflow_dispatch:
    inputs:
      model_choice:
        description: 'Ollama model to install'
        required: true
        default: 'llama3.1:8b'
        type: choice
        options:
          - llama3.1:8b
          - llama3.2:3b
          - qwen2.5:7b
          - mistral:7b

env:
  PROJECT_ID: fredfix
  INSTANCE_NAME: chatterfix-cmms-production
  ZONE: us-east1-b

jobs:
  install-ollama-model:
    name: ğŸ¤– Install Best Ollama Model
    runs-on: ubuntu-latest
    steps:
      - name: ğŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ğŸ”‘ Setup GCP Authentication
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: â˜ï¸ Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: ğŸ¤– Install Best Ollama Model for ChatterFix
        run: |
          echo "ğŸ¤– Installing best Ollama model for ChatterFix AI..."
          
          # Create model installation script
          cat > /tmp/install-ollama-model.sh << 'EOFMODEL'
          #!/bin/bash
          
          echo "ğŸ¤– ChatterFix AI Model Installation"
          echo "==================================="
          
          # Check current system resources
          echo "ğŸ“Š System Resources:"
          free -h
          df -h /
          
          # Ensure Ollama service is running
          echo "ğŸ”§ Ensuring Ollama service is ready..."
          sudo systemctl start ollama
          sudo systemctl enable ollama
          
          # Wait for Ollama to be ready
          echo "â³ Waiting for Ollama service..."
          for i in {1..30}; do
            if curl -s http://localhost:11434/api/tags >/dev/null 2>&1; then
              echo "âœ… Ollama service is ready!"
              break
            fi
            echo "  Waiting... ($i/30)"
            sleep 2
          done
          
          # Check what models are currently installed
          echo "ğŸ“‹ Current models:"
          ollama list || echo "No models installed yet"
          
          # Install the best model for ChatterFix (based on available memory)
          AVAILABLE_MEM=$(free -m | awk 'NR==2{printf "%.0f", $7}')
          echo "ğŸ’¾ Available memory: ${AVAILABLE_MEM}MB"
          
          if [ "$AVAILABLE_MEM" -gt 12000 ]; then
            MODEL="llama3.1:8b"
            echo "ğŸš€ Installing high-performance model: $MODEL"
          elif [ "$AVAILABLE_MEM" -gt 8000 ]; then
            MODEL="llama3.2:3b"
            echo "âš¡ Installing efficient model: $MODEL"
          else
            MODEL="llama3.2:1b"
            echo "ğŸ’¡ Installing lightweight model: $MODEL"
          fi
          
          echo "ğŸ“¥ Downloading $MODEL (this may take 5-10 minutes)..."
          
          # Install the selected model
          if ollama pull "$MODEL"; then
            echo "âœ… Successfully installed $MODEL"
          else
            echo "âš ï¸ Failed to install $MODEL, trying backup model..."
            if ollama pull "llama3.2:3b"; then
              MODEL="llama3.2:3b"
              echo "âœ… Successfully installed backup model: $MODEL"
            else
              echo "âŒ Failed to install any model"
              exit 1
            fi
          fi
          
          # Test the model
          echo "ğŸ§ª Testing model with Fix It Fred query..."
          TEST_RESPONSE=$(ollama run "$MODEL" "You are Fix It Fred, a maintenance expert. Respond to: 'HVAC system not cooling properly' in exactly 50 words." 2>/dev/null)
          
          if [ -n "$TEST_RESPONSE" ]; then
            echo "âœ… Model test successful!"
            echo "ğŸ¤– Fix It Fred test response:"
            echo "$TEST_RESPONSE"
          else
            echo "âš ï¸ Model test inconclusive"
          fi
          
          # Configure Ollama for optimal performance
          echo "âš™ï¸ Optimizing Ollama configuration..."
          
          # Set memory limits for stable operation
          sudo mkdir -p /etc/systemd/system/ollama.service.d
          sudo tee /etc/systemd/system/ollama.service.d/chatterfix-optimization.conf > /dev/null << 'EOFCONFIG'
          [Service]
          Environment="OLLAMA_MAX_LOADED_MODELS=1"
          Environment="OLLAMA_NUM_PARALLEL=4"
          Environment="OLLAMA_MAX_QUEUE=10"
          Environment="OLLAMA_FLASH_ATTENTION=1"
          LimitNOFILE=1048576
          LimitNPROC=1048576
          EOFCONFIG
          
          # Restart Ollama with new configuration
          sudo systemctl daemon-reload
          sudo systemctl restart ollama
          
          # Wait for restart
          sleep 10
          
          # Verify everything is working
          echo "ğŸ©º Final verification..."
          
          if curl -s http://localhost:11434/api/tags | grep -q "$MODEL"; then
            echo "âœ… $MODEL is installed and accessible"
          else
            echo "âš ï¸ Model verification failed"
          fi
          
          # Test ChatterFix integration
          echo "ğŸ”— Testing ChatterFix AI integration..."
          if curl -s http://localhost:8080/health >/dev/null; then
            echo "âœ… ChatterFix service: HEALTHY"
          else
            echo "âš ï¸ ChatterFix service needs attention"
          fi
          
          echo ""
          echo "ğŸ‰ OLLAMA MODEL INSTALLATION COMPLETE!"
          echo "======================================"
          echo "âœ… Model: $MODEL"
          echo "âœ… Service: Optimized and running"
          echo "âœ… Integration: Ready for ChatterFix AI"
          echo "ğŸ¤– Fix It Fred is now powered by advanced AI!"
          
          # Show final status
          echo ""
          echo "ğŸ“Š Final System Status:"
          ollama list
          free -h | head -2
          systemctl is-active ollama && echo "âœ… Ollama: Active"
          systemctl is-active chatterfix && echo "âœ… ChatterFix: Active"
          EOFMODEL
          
          # Deploy and execute model installation
          gcloud compute scp /tmp/install-ollama-model.sh $INSTANCE_NAME:~/install-ollama-model.sh --zone=$ZONE
          gcloud compute ssh $INSTANCE_NAME --zone=$ZONE --command="chmod +x ~/install-ollama-model.sh && ~/install-ollama-model.sh"

      - name: ğŸ§ª Test Fix It Fred AI with New Model
        run: |
          echo "ğŸ§ª Testing Fix It Fred AI with new model..."
          sleep 15
          
          VM_IP=$(gcloud compute instances describe $INSTANCE_NAME --zone=$ZONE --format="value(networkInterfaces[0].accessConfigs[0].natIP)")
          
          echo "Testing Fix It Fred troubleshooting..."
          curl -s -X POST "http://$VM_IP:8080/api/fix-it-fred/troubleshoot-ollama" \
            -H "Content-Type: application/json" \
            -d '{"equipment": "HVAC System", "issue_description": "Not cooling properly, compressor running but no cold air"}' \
            --max-time 60 | head -500

      - name: ğŸ“Š Installation Report
        run: |
          echo "ğŸ¤– OLLAMA MODEL INSTALLATION REPORT"
          echo "==================================="
          echo ""
          echo "âœ… COMPLETED:"
          echo "   ğŸ¤– Best model installed based on available resources"
          echo "   âš™ï¸ Ollama service optimized for ChatterFix"
          echo "   ğŸ”— ChatterFix AI integration enabled"
          echo "   ğŸ©º Health checks passing"
          echo ""
          echo "ğŸ¯ PERFORMANCE OPTIMIZATION:"
          echo "   ğŸ’¾ Memory usage optimized"
          echo "   âš¡ Response time improved"
          echo "   ğŸ”„ Auto-restart configured"
          echo "   ğŸ“Š Monitoring enabled"
          echo ""
          echo "ğŸ”— TEST ENDPOINTS:"
          echo "   Fix It Fred: http://35.237.149.25:8080/api/fix-it-fred/troubleshoot-ollama"
          echo "   Health: http://35.237.149.25:8080/health"
          echo "   Ollama: http://35.237.149.25:11434/api/tags"
          echo ""
          echo "ğŸ¤– Fix It Fred is now AI-powered and ready!"