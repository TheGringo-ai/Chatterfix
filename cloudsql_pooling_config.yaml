# Cloud SQL Connection Pooling Configuration for ChatterFix Phase 6B
# Optimizes database connections for high-performance CMMS operations

apiVersion: v1
kind: ConfigMap
metadata:
  name: cloudsql-pooling-config
  namespace: default
data:
  # Cloud SQL Instance Configuration
  instance_connection_name: "fredfix:us-central1:chatterfix-main"
  database_name: "chatterfix_cmms"
  
  # Connection Pool Settings
  pool_config: |
    min_pool_size: 5
    max_pool_size: 30
    max_overflow: 10
    pool_timeout: 30
    pool_recycle: 3600
    pool_pre_ping: true
    
  # Performance Optimization
  connection_settings: |
    connect_timeout: 10
    command_timeout: 30
    keepalives_idle: 600
    keepalives_interval: 30
    keepalives_count: 3
    
  # SSL and Security
  ssl_config: |
    ssl_mode: "require"
    ssl_cert: "/secrets/client-cert.pem"
    ssl_key: "/secrets/client-key.pem"
    ssl_ca: "/secrets/server-ca.pem"

---
# Cloud Run Service Configuration with Enhanced Database Pooling
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: chatterfix-cmms-optimized
  annotations:
    run.googleapis.com/cloudsql-instances: "fredfix:us-central1:chatterfix-main"
    run.googleapis.com/execution-environment: gen2
spec:
  template:
    metadata:
      annotations:
        autoscaling.knative.dev/minScale: "1"
        autoscaling.knative.dev/maxScale: "10"
        run.googleapis.com/cpu-throttling: "false"
    spec:
      containerConcurrency: 100
      timeoutSeconds: 900
      containers:
      - image: gcr.io/fredfix/chatterfix-cmms:latest
        ports:
        - containerPort: 8080
        env:
        # Database Connection with Pooling
        - name: DB_POOL_SIZE
          value: "20"
        - name: DB_MAX_OVERFLOW
          value: "30"
        - name: DB_POOL_TIMEOUT
          value: "30"
        - name: DB_POOL_RECYCLE
          value: "3600"
        - name: DB_POOL_PRE_PING
          value: "true"
        
        # Cloud SQL Connector Settings
        - name: INSTANCE_CONNECTION_NAME
          value: "fredfix:us-central1:chatterfix-main"
        - name: PRIVATE_IP
          value: "true"
        - name: DB_USER
          valueFrom:
            secretKeyRef:
              name: cloudsql-db-credentials
              key: username
        - name: DB_PASS
          valueFrom:
            secretKeyRef:
              name: cloudsql-db-credentials
              key: password
        
        # Performance Settings
        - name: ENABLE_CONNECTION_POOLING
          value: "true"
        - name: ENABLE_QUERY_CACHE
          value: "true"
        - name: CACHE_TTL_SECONDS
          value: "300"
        
        resources:
          requests:
            memory: "512Mi"
            cpu: "1000m"
          limits:
            memory: "2Gi"
            cpu: "2000m"
        
        # Health Check Configuration
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 60
          timeoutSeconds: 10
          
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 10
          timeoutSeconds: 5

---
# Python Connection Pooling Implementation
apiVersion: v1
kind: ConfigMap
metadata:
  name: python-pooling-code
data:
  connection_pool.py: |
    """
    Enhanced Cloud SQL Connection Pooling for ChatterFix CMMS
    Implements high-performance database connections with automatic failover
    """
    
    import os
    import logging
    from sqlalchemy import create_engine, event
    from sqlalchemy.pool import QueuePool
    from google.cloud.sql.connector import Connector
    import psycopg2
    from contextlib import contextmanager
    
    logger = logging.getLogger(__name__)
    
    class CloudSQLConnectionPool:
        def __init__(self):
            self.connector = Connector()
            self.engine = None
            self.setup_connection_pool()
            
        def setup_connection_pool(self):
            """Initialize optimized connection pool"""
            
            # Connection parameters
            instance_connection_name = os.getenv("INSTANCE_CONNECTION_NAME", "fredfix:us-central1:chatterfix-main")
            db_user = os.getenv("DB_USER", "postgres")
            db_pass = os.getenv("DB_PASS", "REDACTED_DB_PASSWORD")
            db_name = os.getenv("DB_NAME", "chatterfix_cmms")
            
            # Pool configuration
            pool_size = int(os.getenv("DB_POOL_SIZE", "20"))
            max_overflow = int(os.getenv("DB_MAX_OVERFLOW", "30"))
            pool_timeout = int(os.getenv("DB_POOL_TIMEOUT", "30"))
            pool_recycle = int(os.getenv("DB_POOL_RECYCLE", "3600"))
            
            def getconn():
                conn = self.connector.connect(
                    instance_connection_name,
                    "pg8000",
                    user=db_user,
                    password=db_pass,
                    db=db_name,
                    timeout=30,
                    refresh_timeout=30
                )
                return conn
                
            # Create engine with optimized pooling
            self.engine = create_engine(
                "postgresql+pg8000://",
                creator=getconn,
                poolclass=QueuePool,
                pool_size=pool_size,
                max_overflow=max_overflow,
                pool_timeout=pool_timeout,
                pool_recycle=pool_recycle,
                pool_pre_ping=True,
                echo=False
            )
            
            # Add connection event listeners
            @event.listens_for(self.engine, "connect")
            def set_sqlite_pragma(dbapi_connection, connection_record):
                logger.info("New database connection established")
                
            @event.listens_for(self.engine, "checkout")
            def checkout_listener(dbapi_connection, connection_record, connection_proxy):
                logger.debug("Connection checked out from pool")
                
            logger.info(f"Connection pool initialized: size={pool_size}, max_overflow={max_overflow}")
            
        @contextmanager
        def get_connection(self):
            """Get database connection from pool with automatic cleanup"""
            connection = None
            try:
                connection = self.engine.connect()
                yield connection
            except Exception as e:
                if connection:
                    connection.rollback()
                logger.error(f"Database connection error: {e}")
                raise
            finally:
                if connection:
                    connection.close()
                    
        def execute_query(self, query, params=None):
            """Execute query with connection pooling and error handling"""
            try:
                with self.get_connection() as conn:
                    result = conn.execute(query, params or {})
                    return result.fetchall()
            except Exception as e:
                logger.error(f"Query execution failed: {e}")
                raise
                
        def execute_transaction(self, queries):
            """Execute multiple queries in a transaction"""
            try:
                with self.get_connection() as conn:
                    trans = conn.begin()
                    try:
                        results = []
                        for query, params in queries:
                            result = conn.execute(query, params or {})
                            results.append(result.fetchall())
                        trans.commit()
                        return results
                    except Exception:
                        trans.rollback()
                        raise
            except Exception as e:
                logger.error(f"Transaction failed: {e}")
                raise
                
        def close(self):
            """Close all connections and cleanup"""
            if self.engine:
                self.engine.dispose()
            if self.connector:
                self.connector.close()
            logger.info("Connection pool closed")
    
    # Global connection pool instance
    connection_pool = CloudSQLConnectionPool()

  redis_cache.py: |
    """
    Redis Caching Layer for Heavy Database Queries
    """
    
    import redis
    import json
    import logging
    from typing import Optional, Any
    from datetime import timedelta
    
    logger = logging.getLogger(__name__)
    
    class RedisCache:
        def __init__(self):
            self.redis_client = None
            self.setup_redis()
            
        def setup_redis(self):
            """Initialize Redis connection"""
            try:
                redis_host = os.getenv("REDIS_HOST", "localhost")
                redis_port = int(os.getenv("REDIS_PORT", "6379"))
                redis_db = int(os.getenv("REDIS_DB", "0"))
                
                self.redis_client = redis.Redis(
                    host=redis_host,
                    port=redis_port,
                    db=redis_db,
                    decode_responses=True,
                    socket_timeout=5,
                    socket_connect_timeout=5,
                    retry_on_timeout=True
                )
                
                # Test connection
                self.redis_client.ping()
                logger.info("Redis cache connected successfully")
                
            except Exception as e:
                logger.warning(f"Redis not available, falling back to no cache: {e}")
                self.redis_client = None
                
        def get(self, key: str) -> Optional[Any]:
            """Get cached value"""
            if not self.redis_client:
                return None
                
            try:
                value = self.redis_client.get(key)
                if value:
                    return json.loads(value)
                return None
            except Exception as e:
                logger.error(f"Cache get error for key {key}: {e}")
                return None
                
        def set(self, key: str, value: Any, ttl: int = 300):
            """Set cached value with TTL"""
            if not self.redis_client:
                return False
                
            try:
                serialized = json.dumps(value, default=str)
                self.redis_client.setex(key, ttl, serialized)
                return True
            except Exception as e:
                logger.error(f"Cache set error for key {key}: {e}")
                return False
                
        def delete(self, key: str):
            """Delete cached value"""
            if not self.redis_client:
                return False
                
            try:
                self.redis_client.delete(key)
                return True
            except Exception as e:
                logger.error(f"Cache delete error for key {key}: {e}")
                return False
                
        def clear_pattern(self, pattern: str):
            """Clear all keys matching pattern"""
            if not self.redis_client:
                return False
                
            try:
                keys = self.redis_client.keys(pattern)
                if keys:
                    self.redis_client.delete(*keys)
                return True
            except Exception as e:
                logger.error(f"Cache clear pattern error for {pattern}: {e}")
                return False
    
    # Global cache instance
    cache = RedisCache()