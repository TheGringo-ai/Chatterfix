# Cloud Build configuration for ChatterFix CMMS with embedded AI
steps:
  # Build the AI-enhanced Docker image
  - name: 'gcr.io/cloud-builders/docker'
    args: [
      'build',
      '-f', 'Dockerfile.ai-enhanced',
      '-t', 'gcr.io/$PROJECT_ID/chatterfix-ai-enhanced:latest',
      '-t', 'gcr.io/$PROJECT_ID/chatterfix-ai-enhanced:$BUILD_ID',
      '.'
    ]
    timeout: '1200s'  # Allow extra time for LLaMA model downloads

  # Push the image
  - name: 'gcr.io/cloud-builders/docker'
    args: ['push', 'gcr.io/$PROJECT_ID/chatterfix-ai-enhanced:latest']

  # Deploy to Cloud Run with enhanced configuration
  - name: 'gcr.io/cloud-builders/gcloud'
    args: [
      'run', 'deploy', 'chatterfix-ai-enhanced',
      '--image', 'gcr.io/$PROJECT_ID/chatterfix-ai-enhanced:latest',
      '--region', 'us-central1',
      '--platform', 'managed',
      '--memory', '4Gi',  # Increased memory for AI models
      '--cpu', '2',       # Increased CPU for AI processing
      '--max-instances', '10',
      '--min-instances', '1',  # Keep one instance warm
      '--timeout', '300s',
      '--concurrency', '10',
      '--allow-unauthenticated',
      '--set-env-vars', 'AI_PROVIDER_PRIORITY=llama,grok,groq,openai,anthropic,google,cohere,fallback',
      '--set-env-vars', 'LLAMA_SERVER=http://localhost:11434',
      '--set-env-vars', 'LLAMA_MODEL=llama3.2:1b',
      '--set-env-vars', 'AI_MAX_TOKENS=4096',
      '--set-env-vars', 'AI_REQUEST_TIMEOUT=60'
    ]

# Build options for better performance
options:
  machineType: 'E2_HIGHCPU_8'  # High CPU for AI model processing
  diskSizeGb: 50               # Extra disk space for models
  logging: CLOUD_LOGGING_ONLY

# Build timeout
timeout: '1500s'